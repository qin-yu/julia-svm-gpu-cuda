# Julia SVM Helper Functions
```julia
##############################
## by Qin Yu, Feb 2019
## using Julia 1.0.3
##############################

gaussian_kernel_matrix(𝑋, γ) =  # 𝑋's rows are input tensors.
    exp.(- γ * pairwise(SqEuclidean(), Matrix(𝑋), dims=1))
gaussian_kernel_matrix(𝑋, 𝑋_test, γ) =  # 𝑋's rows are input tensors.
    exp.(- γ * pairwise(SqEuclidean(), Matrix(𝑋_test), Matrix(𝑋), dims=1))

function extract_each_classes(𝑋_ALL, 𝒚_ALL)
    CLASSES = sort(unique(𝒚_ALL))
    NO_OF_CLASSES = length(CLASSES)
    L = [sum(𝒚_ALL .== class) for class in CLASSES]
    𝑋_EACH_train = [𝑋_ALL[:,:,𝒚_ALL .== class] for class in CLASSES]
    𝑋s = [hcat(transpose(reshape(𝑋_EACH_train[i], 28*28, L[i])), ones(L[i])) for i = 1:NO_OF_CLASSES]
    𝒚s = ones.(Int, L) .* CLASSES
    return L, 𝑋s, 𝒚s
end

function extract_all_test(𝑋_ALL, 𝒚_ALL)
    𝑋s = hcat(transpose(reshape(𝑋_ALL, 28*28, :)), ones(size(𝑋_ALL, 3)))
    return 𝑋s, 𝒚_ALL
end

function prepare_one_vs_rest(𝑋_ALL, 𝒚_ALL)
    𝑋 = hcat(transpose(reshape(𝑋_ALL, 28*28, :)), ones(size(𝑋_ALL, 3)))
    𝒚s = [(y -> y == pos_label ? 1 : -1).(𝒚_ALL) for pos_label = 0:9]
    return 𝑋, 𝒚s
end

function prepare_MNIST_data(class_pos, class_neg, γ)
    # Load train data
    𝑋_ALL_train, 𝒚_ALL_train = MNIST.traindata(Float32)

    NO_OF_NEGATIVE = sum(𝒚_ALL_train .== class_neg)
    NO_OF_POSITIVE = sum(𝒚_ALL_train .== class_pos)
    l = NO_OF_NEGATIVE + NO_OF_POSITIVE

    𝑋_neg_train = 𝑋_ALL_train[:,:,𝒚_ALL_train .== class_neg]
    𝑋_pos_train = 𝑋_ALL_train[:,:,𝒚_ALL_train .== class_pos]
    𝒚_neg_train = - ones(Int, NO_OF_NEGATIVE)
    𝒚_pos_train =   ones(Int, NO_OF_POSITIVE)

    randomisation = randperm(l)
    𝑋 = hcat(transpose(reshape(cat(𝑋_neg_train, 𝑋_pos_train; dims=3), 28*28, l)), ones(l))[randomisation,:]
    𝒚 = vcat(𝒚_neg_train, 𝒚_pos_train)[randomisation,:]

    gaussian_kernel_matrix(𝑋, γ) = exp.(- γ * pairwise(SqEuclidean(), Matrix(𝑋), dims=1))  # 𝑋's rows are input tensors.
    K = gaussian_kernel_matrix(𝑋, γ)

    # Load test data
    𝑋_ALL_test,  𝒚_ALL_test  = MNIST.testdata(Float32)

    NO_OF_NEGATIVE_test = sum(𝒚_ALL_test .== class_neg)
    NO_OF_POSITIVE_test = sum(𝒚_ALL_test .== class_pos)
    l_test = NO_OF_NEGATIVE_test + NO_OF_POSITIVE_test

    𝑋_neg_test = 𝑋_ALL_test[:,:,𝒚_ALL_test .== class_neg]
    𝑋_pos_test = 𝑋_ALL_test[:,:,𝒚_ALL_test .== class_pos]
    𝒚_neg_test = - ones(Int, NO_OF_NEGATIVE_test)
    𝒚_pos_test =   ones(Int, NO_OF_POSITIVE_test)

    𝑋_test = hcat(transpose(reshape(cat(𝑋_neg_test, 𝑋_pos_test; dims=3), 28*28, l_test)), ones(l_test))
    𝒚_test = vcat(𝒚_neg_test, 𝒚_pos_test)

    gaussian_kernel_matrix(𝑋, 𝑋_test, γ) = exp.(- γ * pairwise(SqEuclidean(), Matrix(𝑋_test), Matrix(𝑋), dims=1))
    K_test = gaussian_kernel_matrix(𝑋, 𝑋_test, γ)

    return (𝑋, 𝒚, K, l), (𝑋_test, 𝒚_test, K_test, l_test)
end

# Only as termination criterion for the whole process.
function monitor_kkt_condition(𝜶, 𝝈, 𝒚; accuracy=1e-2, 𝐶::Int32=Int32(1))
    𝑦𝑓𝒙 = 𝒚 .* 𝝈
    for (idx,α) in enumerate(𝜶)
        if α == 0 && 𝑦𝑓𝒙[idx] < 1
            return false
        elseif α == 𝐶 && 𝑦𝑓𝒙[idx] > 1
            return false
        elseif 0 < α < 𝐶 && !isapprox(𝑦𝑓𝒙[idx], 1; atol=accuracy)
            return false
        end
    end
    return true
end

function select_new_points(𝜶, 𝝈, 𝒚, 𝑀, support_vector_idx; accuracy=1e-2, 𝐶::Int32=Int32(1))
    𝑦𝑓𝒙 = 𝒚 .* 𝝈
    list_of_new_ids = Int[]
    list_of_new_min = Float64[]
    for (idx,α) in enumerate(𝜶)
        if (α == 0 && 𝑦𝑓𝒙[idx] < 1) || (α == 𝐶 && 𝑦𝑓𝒙[idx] > 1) || (0 < α < 𝐶 && !isapprox(𝑦𝑓𝒙[idx], 1; atol=accuracy))
            push!(list_of_new_ids, idx)
            push!(list_of_new_min, 𝑦𝑓𝒙[idx])
        end
    end
    list_of_new_ids = list_of_new_ids[sortperm(list_of_new_min)]
    setdiff!(list_of_new_ids, support_vector_idx)
    return list_of_new_ids
end
```


# Julia SVM
```julia
##############################
## by Qin Yu, Apr 2019
## using Julia 1.1.0
##############################

using Revise, BenchmarkTools                           # Development
using JLD2, FileIO, MLDatasets                         # Data & IO
using LinearAlgebra, Distances, Random, Distributions  # Maths
using CUDAdrv, CUDAnative, CuArrays                    # GPU

MAX_MINI_BATCH_ID = 50

#################### Compute Adatron SGD in GPU: ####################
@inline sync_threads_and(predicate::Int32) = ccall("llvm.nvvm.barrier0.and", llvmcall, Int32, (Int32,), predicate)
@inline sync_threads_and(predicate::Bool) = ifelse(sync_threads_and(Int32(predicate)) !== Int32(0), true, false)

function kernel_soft_SGD_SVM(𝜶, 𝝈, K, 𝒚, l::Int32, 𝐶::Int32)
    j = (blockIdx().x-1) * blockDim().x + threadIdx().x
    can_stop = false
    δᵢ = @cuStaticSharedMem(Float32, 1)
    while !sync_threads_and(can_stop)
        last_α_j = 𝜶[j]
        # Adatron:
        for i = 1:l
            if j == i  # Online
                last_α = 𝜶[i]
                𝜇ᵢ = 1 / K[i,i]
                δᵢ[1] = 𝜇ᵢ * (1 - 𝒚[i] * 𝝈[i])
                𝜶[i] = 𝜶[i] + δᵢ[1]
                𝜶[i] < 0 && (𝜶[i] = 0; δᵢ[1] = 0 - last_α)
                𝜶[i] > 𝐶 && (𝜶[i] = 𝐶; δᵢ[1] = 𝐶 - last_α)
            end
            sync_threads()
            𝝈[j] += δᵢ[1] * 𝒚[i] * K[i,j]  # Parallel update
        end
        # Stopping criterion:
        can_stop = false
        isapprox(𝜶[j], last_α_j; atol=1e-4) && (can_stop = true)
    end
    return nothing
end

function optimise_working_set(𝜶, 𝝈, K, 𝒚; 𝐶::Int32=Int32(1))
    l = Int32(length(𝜶))
    cu_𝜶 = CuArray{Float32}(𝜶)
    cu_𝝈 = CuArray{Float32}(𝝈)
    cu_K = CuArray{Float32}(K)
    cu_𝒚 = CuArray{Float32}(𝒚)

    @cuda threads=l kernel_soft_SGD_SVM(cu_𝜶, cu_𝝈, cu_K, cu_𝒚, l, 𝐶)

    𝜶 = Array{Float32}(cu_𝜶)
    return 𝜶
end

function optimise_working_set_CPU(𝜶, 𝝈, K, 𝒚; 𝐶=1)
    l = length(𝜶)
    while true
        last_𝜶 = copy(𝜶)
        for i = 1:l
            𝜇ᵢ = 1 / K[i,i]
            δᵢ = 𝜇ᵢ * (1 - 𝒚[i] * 𝝈[i])
            𝜶[i] = 𝜶[i] + δᵢ
            𝜶[i] < 0 && (𝜶[i] = 0; δᵢ = 0 - last_𝜶[i])
            𝜶[i] > 𝐶 && (𝜶[i] = 𝐶; δᵢ = 𝐶 - last_𝜶[i])
            𝝈 .+= δᵢ * 𝒚[i] * K[i,:]
        end
        all(isapprox.(last_𝜶, 𝜶; atol=1e-4)) && break
    end
    return 𝜶
end

function stochastic_decomposition_test(𝐶, 𝑀, ACCURACY, l, 𝒚, K, l_test, 𝒚_test, K_test; usecpu=false)
    𝑀_safe = div(𝑀, 4) * 3
    𝑀_rand = 𝑀 - 𝑀_safe

    𝜶 = zeros(Float32, l)
    𝝈 = zeros(Float32, l)

    selected_new_idx = []
    support_vector_idx, alpha0_vector_idx = collect(1:𝑀), collect(𝑀+1:l)
    working_set_counter = zeros(Int32, l)

    batch_id = 0
    while !monitor_kkt_condition(𝜶, 𝝈, 𝒚; 𝐶=𝐶) && batch_id < MAX_MINI_BATCH_ID
        batch_id += 1

        current_working_set_counter = working_set_counter[support_vector_idx]
        sorted_support_vector_idx = support_vector_idx[sortperm(current_working_set_counter)]

        if length(support_vector_idx) >= 𝑀_safe
            no_of_new_idx = min(length(selected_new_idx), 𝑀_rand)
        else
            no_of_new_idx = min(length(selected_new_idx), 𝑀-length(support_vector_idx))
        end

        no_of_old_idx = length(support_vector_idx)
        no_of_exceeds = no_of_new_idx + no_of_old_idx - 𝑀
        if no_of_exceeds > 0
            abandoned_support_vector_idx = sorted_support_vector_idx[end+1-no_of_exceeds:end]
            setdiff!(support_vector_idx, abandoned_support_vector_idx)
            union!(alpha0_vector_idx, abandoned_support_vector_idx)
        end
        selected_new_idx = selected_new_idx[1:no_of_new_idx]
        union!(support_vector_idx, selected_new_idx)
        setdiff!(alpha0_vector_idx, selected_new_idx)

        working_set_counter[support_vector_idx] .+= 1
        𝜶_subset, 𝝈_subset = 𝜶[support_vector_idx], 𝝈[support_vector_idx]
        𝒚_subset, K_subset = 𝒚[support_vector_idx], K[support_vector_idx,support_vector_idx]
        if usecpu
            𝜶[support_vector_idx] = optimise_working_set_CPU(𝜶_subset, 𝝈_subset, K_subset, 𝒚_subset; 𝐶=𝐶)
        else
            𝜶[support_vector_idx] = optimise_working_set(𝜶_subset, 𝝈_subset, K_subset, 𝒚_subset; 𝐶=𝐶)
        end

        𝝈 = K * (𝜶 .* 𝒚)

        for (i_index,i) in enumerate(support_vector_idx)
            if 𝜶[i] == 0
                deleteat!(support_vector_idx, i_index)
                push!(alpha0_vector_idx, i)
            end
        end

        selected_new_idx = select_new_points(𝜶, 𝝈, 𝒚, 𝑀, support_vector_idx; accuracy=ACCURACY, 𝐶=𝐶)
    end

    𝒚̂      = sign.(𝝈)
    𝒚̂_test = sign.(K_test * (𝜶 .* 𝒚))
    error_rate      = count(𝒚̂ .!= 𝒚) / l * 100
    error_rate_test = count(𝒚̂_test .!= 𝒚_test) / l_test * 100
    return 𝜶, error_rate, error_rate_test, monitor_kkt_condition(𝜶, 𝝈, 𝒚; 𝐶=𝐶)
end

function stochastic_decomposition(𝐶, 𝑀, ACCURACY, l, 𝒚, K; usecpu=false)
    𝑀_safe = div(𝑀, 4) * 3
    𝑀_rand = 𝑀 - 𝑀_safe

    𝜶 = zeros(Float32, l)
    𝝈 = zeros(Float32, l)

    selected_new_idx = []
    support_vector_idx, alpha0_vector_idx = collect(1:𝑀), collect(𝑀+1:l)
    working_set_counter = zeros(Int32, l)

    batch_id = 0
    while !monitor_kkt_condition(𝜶, 𝝈, 𝒚; accuracy=ACCURACY, 𝐶=𝐶) && batch_id < MAX_MINI_BATCH_ID
        batch_id += 1

        current_working_set_counter = working_set_counter[support_vector_idx]
        sorted_support_vector_idx = support_vector_idx[sortperm(current_working_set_counter)]

        if length(support_vector_idx) >= 𝑀_safe
            no_of_new_idx = min(length(selected_new_idx), 𝑀_rand)
        else
            no_of_new_idx = min(length(selected_new_idx), 𝑀-length(support_vector_idx))
        end

        no_of_old_idx = length(support_vector_idx)
        no_of_exceeds = no_of_new_idx + no_of_old_idx - 𝑀
        if no_of_exceeds > 0
            abandoned_support_vector_idx = sorted_support_vector_idx[end+1-no_of_exceeds:end]
            setdiff!(support_vector_idx, abandoned_support_vector_idx)
            union!(alpha0_vector_idx, abandoned_support_vector_idx)
        end
        selected_new_idx = selected_new_idx[1:no_of_new_idx]
        union!(support_vector_idx, selected_new_idx)
        setdiff!(alpha0_vector_idx, selected_new_idx)

        working_set_counter[support_vector_idx] .+= 1
        𝜶_subset, 𝝈_subset = 𝜶[support_vector_idx], 𝝈[support_vector_idx]
        𝒚_subset, K_subset = 𝒚[support_vector_idx], K[support_vector_idx,support_vector_idx]
        if usecpu
            println("using CPU")
            𝜶[support_vector_idx] = optimise_working_set_CPU(𝜶_subset, 𝝈_subset, K_subset, 𝒚_subset; 𝐶=𝐶)
        else
            𝜶[support_vector_idx] = optimise_working_set(𝜶_subset, 𝝈_subset, K_subset, 𝒚_subset; 𝐶=𝐶)
        end
        𝝈 = K * (𝜶 .* 𝒚)

        for (i_index,i) in enumerate(support_vector_idx)
            if 𝜶[i] == 0
                deleteat!(support_vector_idx, i_index)
                push!(alpha0_vector_idx, i)
            end
        end

        selected_new_idx = select_new_points(𝜶, 𝝈, 𝒚, 𝑀, support_vector_idx; accuracy=ACCURACY, 𝐶=𝐶)
    end

    𝒚̂ = sign.(𝝈)
    error_rate = count(𝒚̂ .!= 𝒚) / l * 100
    return 𝜶, error_rate, monitor_kkt_condition(𝜶, 𝝈, 𝒚; accuracy=ACCURACY, 𝐶=𝐶)
end
```

# MNIST Training and Testing
```julia
##############################
## by Qin Yu, Apr 2019
## using Julia 1.1.0
##############################

#################### MNIST One-vs-One Multi-class ####################
function MNIST_train(L, L_test, 𝑋s, 𝒚s, 𝐶, 𝑀, ACCURACY, γ; usecpu=false)
    time_K_spent = time_GPU_spent = time_total_spent = 0
    time_total_start = time()

    randomisation_list = Array{Int32,1}[]
    𝜶_list = Array{Float32,1}[]
    error_list = Float64[]
    error_list_test = Float64[]
    finished_list = Bool[]
    time_ivj_list = Float64[]

    for ci = 1:10, cj = 1:10
        ci >= cj && continue
        print("$(ci-1)-vs-$(cj-1) - ")

        time_K_start = time()
        l = L[ci] + L[cj]
        randomisation = randperm(l)
        𝒚 = vcat(ones(Int32, size(𝒚s[ci])), -ones(Int32, size(𝒚s[cj])))[randomisation,:]
        𝑋 = vcat(𝑋s[ci], 𝑋s[cj])[randomisation,:]
        K = gaussian_kernel_matrix(𝑋, γ)
        time_K_spent += time() - time_K_start

        # l_test = L_test[ci] + L_test[cj]
        # randomisation_test = randperm(l_test)
        # 𝒚_test = vcat(ones(Int32, size(𝒚s_test[ci])), -ones(Int32, size(𝒚s_test[cj])))[randomisation_test,:]
        # 𝑋_test = vcat(𝑋s_test[ci], 𝑋s_test[cj])[randomisation_test,:]
        # K_test = gaussian_kernel_matrix(𝑋, 𝑋_test, γ)

        time_GPU_start = time()
        𝜶, error_rate, finished = stochastic_decomposition(𝐶, 𝑀, ACCURACY, l, 𝒚, K; usecpu=usecpu)
        # 𝜶, error_rate, error_rate_test, finished =
        #     stochastic_decomposition(𝐶, 𝑀, l, 𝒚, K, l_test, 𝒚_test, K_test)
        time_GPU_spent_this = time() - time_GPU_start
        push!(time_ivj_list, time_GPU_spent_this)
        time_GPU_spent += time_GPU_spent_this

        println("error rate = $error_rate% ; finished = $finished ; time = $time_GPU_spent_this")
        # println("error rate = $error_rate% ; test error rate = $error_rate_test% ;
        #     finished = $finished ; time = $time_GPU_spent_this")

        push!(randomisation_list, randomisation)
        push!(𝜶_list, 𝜶)
        push!(error_list, error_rate)
        # push!(error_list_test, error_rate_test)
        push!(finished_list, finished)
    end

    time_total_spent = time() - time_total_start
    return ([time_K_spent, time_GPU_spent, time_total_spent],
            randomisation_list, 𝜶_list, error_list, error_list_test, finished_list, time_ivj_list)
end

function MNIST_test_slave(𝑋_ALL, 𝒚_ALL, 𝑋s, 𝒚s, 𝜶_list, randomisation_list, γ)
    i = 0
    𝒚̂_test_list = Array{Int}[]
    for ci = 1:10, cj = 1:10
        ci >= cj && continue
        print("$(ci-1)-vs-$(cj-1) - ")
        i += 1

        𝑋_test, 𝒚_test = extract_all_test(𝑋_ALL, 𝒚_ALL)
        K_test = gaussian_kernel_matrix(vcat(𝑋s[ci], 𝑋s[cj])[randomisation_list[i],:], 𝑋_test, γ)
        𝒚_true = vcat(ones(Int32, size(𝒚s[ci])), -ones(Int32, size(𝒚s[cj])))[randomisation_list[i],:]
        𝝈_test = K_test * (𝜶_list[i] .* 𝒚_true)
        𝒚̂_test = (y -> y > 0 ? ci-1 : cj-1).(𝝈_test)

        push!(𝒚̂_test_list, 𝒚̂_test)
    end
    return 𝒚̂_test_list
end

function MNIST_test(𝒚_test, 𝒚̂_test_list)
    𝒚̂_test_matrix = hcat(𝒚̂_test_list...)
    𝒚̂_test = mapslices(mode, 𝒚̂_test_matrix; dims=2)
    error_rate_test = count(𝒚̂_test .!= 𝒚_test) / length(𝒚_test) * 100
    correct_rate_test = 100 - error_rate_test  # First run: 99.1%
    return 𝒚̂_test, error_rate_test, correct_rate_test
end

function run(𝐶::Int32=Int32(1),    # Penalty
              𝑀::Int=512,          # Max size of minibatch
              ACCURACY=1e-2,       # GPU accuracy = 0.01 * ACCURACY
              γ=0.015;             # Gaussian kernel parameter
              usecpu=false)
    𝑋_ALL, 𝒚_ALL = MNIST.traindata(Float32)
    𝑋_ALL_test, 𝒚_ALL_test = MNIST.testdata(Float32)
    L, 𝑋s, 𝒚s = extract_each_classes(𝑋_ALL, 𝒚_ALL)
    L_test, 𝑋s_test, 𝒚s_test = extract_each_classes(𝑋_ALL_test,  𝒚_ALL_test)

    @time time_list, randomisation_list, 𝜶_list, error_list, error_list_test, finished_list, time_ivj_list =
        MNIST_train(L, L_test, 𝑋s, 𝒚s, 𝐶, 𝑀, ACCURACY, γ; usecpu=usecpu)
    @save "1v1.jld2" time_list, randomisation_list, 𝜶_list, error_list, error_list_test, finished_list, time_ivj_list
    @time 𝒚̂_test_list = MNIST_test_slave(𝑋_ALL, 𝒚_ALL, 𝑋s, 𝒚s, 𝜶_list, randomisation_list, γ)
    @time 𝒚̂_test, error_rate_test = MNIST_test(𝒚_ALL, 𝒚̂_test_list)
    println(error_rate_test)
end
```
